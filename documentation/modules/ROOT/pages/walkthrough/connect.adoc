= Demo Walkthrough
include::../_attributes.adoc[]

[#examplekafkaconnect]
== Kafka Connect Introduction

[IMPORTANT]
====
This section assumes you have setup a kafka cluster in `{test-project}` already.  If you have not, then please first complete xref::walkthrough/strimzi.adoc[this part of the walkthrough]
====

. Go to the `{test-project}` project, Developer Perspective
+
NOTE: The `example` kafka cluster should be fully running now
+
. Before doing anything, pick a shell and run the following `stern` command which we'll use to look at the plugins on the Connect instance
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
stern connect -n {test-project} -i "Added plugin"
----
--
====
+
. Click the book with the plus (`Quick Catalog Add`) and type `Connect` in the text box and then click `Create`
+
image:kafka-connect-catalog.png[]
+
. Then fill in the kafka connect form view per below **being extra careful to remove the tls certs** and **add the annotation* in YAML
+ 
image:kafka-connect-form.png[]
image:remove-certs.png[]
+
. Switch to YAML view and make sure to add the following annotation:
+
----
  annotations:
    strimzi.io/use-connector-resources: 'true'
----
+
image::kafka-connect-yaml.png[]
+
. Finally click `Create` to create the KakfaConnect
. After a few moments you should see the following in the `stern` shell (*Terminal 1*)
+
[.console-output]
----
+ example-connect-7856c88cf-mthmb â€º example-connect
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,423 INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,423 INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,424 INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,424 INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
#example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,424 INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]#
...
NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
----
. Then, make the following two terminals visible and run the commands therein:
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal2::
+
--
Here we will remote to the connect instance

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc rsh deploy/example-connect
----

Once connected, run the following in the pod:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
cd /tmp
echo "Line 1" >> test.txt
----
--
Terminal 3::
+
--
Here we will look to receive any updates on the topic that the file will be streaming to

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it example-kafka-0 -n {test-project} -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic {example-connector-topic} --from-beginning
----
--
====
. Next, use the `Quick Add Catalog` button to create a new `Kafka Connector`
+
image:new-kafka-connector.png[]
+
. Switch to `YAML View`
. Show that the name of the connector matches the Plugin log output from Terminal 1
** `org.apache.kafka.connect.file.FileStreamSourceConnector`
. Update the YAML as follows:
** Make sure the topic matches what is being consumed in *Terminal 3*
** Make sure the file matches what was just created in the `rsh` on *Terminal 2*
+
image::file-connector-yaml.png[]
+
. A few moments later, you should then see the following in *Terminal 3*
+
[.console-output]
----
{"schema":{"type":"string","optional":false},"payload":"Line 1"}
----
+
. Click `Search` and then start typing `kafkaconnnect` and select `KafkaConnector` from the drop-down
+
image:connector-search.png[]
+
. Click on the `file-stream-connector` and then the YAML view
+
image::connector-status.png[]
+
[NOTE]
====
Alternatively, you can following command in Terminal 2 (rsh)

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
curl localhost:8083/connectors/file-stream-connector/status
----
====
. Next let's pump some text through the file
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 2 (rsh)::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
c=0; while (true); do ((c=c+1)); echo "Line ${c}" >> test.txt; sleep 2; done
----
--
====
+
. This should cause lines like the following to appear every 2 seconds in *Terminal 3*
+
[.console-output]
----
{"schema":{"type":"string","optional":false},"payload":"Line 2"}
{"schema":{"type":"string","optional":false},"payload":"Line 3"}
{"schema":{"type":"string","optional":false},"payload":"Line 4"}
{"schema":{"type":"string","optional":false},"payload":"Line 5"}
{"schema":{"type":"string","optional":false},"payload":"Line 6"}
{"schema":{"type":"string","optional":false},"payload":"Line 7"}
{"schema":{"type":"string","optional":false},"payload":"Line 8"}
{"schema":{"type":"string","optional":false},"payload":"Line 9"}
{"schema":{"type":"string","optional":false},"payload":"Line 10"}
----
+
. Delete the kafkaconnector and then notice that eventually (when the amq operator catches up) the topic goes silent
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc delete -n {test-project} kafkaconnector/file-stream-connector
----
--
====

[#cdc]
== Legacy App Change Data Capture

In this section we're going to put all the pieces together and demonstrate the ability to have our website updated based on debezium and a consumer that we write.  This is the general plan:

image::cdc-arch-overview.png[]

. Remember that we've already uploaded a number of items and quantities into the inventory via the legacy app
+
image:loaded-inventory.png[]
+
. First we will deploy the Strimzi-ified KafkaConnect and Debezium Connector for our application.  Show them briefly this playbook:
+
.link:{github-repo}/{resources-repo}/build-resources.yaml[build-resources.yaml^]
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$provision_debezium.yaml[]
----
+
. To deploy the playbook, run the following in the shell
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
ansible-playbook -i pass:[${DEMO_HOME}]/ansible/demo/inventory \
    pass:[${DEMO_HOME}]/ansible/demo/main.yaml \
    -e "ACTION=debezium_create" 
----
+
. From the Developer Perspective, watch the DBZ UI and the connector appear
+
. Search for the KafkaConnect for `debezium` as shown:
+
image::search-dbz-connect.png[]
+
. Review the yaml (see below for example from playbook)
** Point out all the possible connectors that can be registered within this connect (for all the different databases)
+
.link:{github-repo}/{resources-repo}/build-resources.yaml[build-resources.yaml^]
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$kafka-connect.yaml.j2[]
----
+
. Click on the link badge of the `dbz-ui` and show the visualization of the debezium connector:
+
image::dbz-ui-badge.png[]
image::dbz-ui.png[]
+
. Search for the `KafkaConnector` and click on the `debezium-connector` 
+
image:search-dbz-connector.png[]
+
. Review the YAML (see below for example from playbook)
** Point out the legacy topic we want to watch
+
.link:{github-repo}/{resources-repo}/build-resources.yaml[build-resources.yaml^]
[source,yaml,subs="+macros,attributes+"]
----
include::ROOT:example$kafka-connector-mssql.yaml.j2[]
----
+
. Now let's watch that legacy topic that we see in the connector.  We'd expect there should be data there for the one row we added to the legacy database
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n {kafka-project} -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic {cdc-topic} --from-beginning
----
--
====
+
. After a few moments, a blob of JSON should appear that looks something like the following (just unformatted):
+
.link:{github-repo}/{resources-repo}/build-resources.yaml[build-resources.yaml^]
[source,json,subs="+macros,attributes+"]
----
include::ROOT:example$testCreateEvent-1.15.json[]
----
+
. Copy the JSON blob and paste it into an online JSON formatting service such as link:https://jsonformatter.org/[JSON Formatter]
+
image:json-formatter.png[]
+
. Use folding ability to point out the following parts of the message:
** `schema`
** `payload` 
** `before` and `after`
** `op`
+
image:schema-payload.png[]

[#deploylegacyconsumer]
=== Deploy Legacy Consumer

Run the following ansible command to create a deployment and configmap for the connector

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
ansible-playbook -i pass:[${DEMO_HOME}]/ansible/demo/inventory \
    pass:[${DEMO_HOME}]/ansible/demo/main.yaml \
    -e "ACTION=consumer_create" 
----

[#debeziumconnector]
== Enable Debezium Connector

=== Configure SQL Database

See last part of `ansible/demo/templates/configmap-data-sql.yaml.j2` and instructions link:https://debezium.io/documentation/reference/connectors/sqlserver.html#_enabling_cdc_on_the_sql_server_database[here]

[#demonstratedbz]
== Demonstrate Debezium

. Connect to database using Adminer per instructions xref:03-appendix.adoc#mssql[here]
. In another terminal watch where the order events will go
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc exec -it demo-kafka-0 -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mssql-server-linux.dbo.Orders --from-beginning
----
+
. Go to adminer and create a new entry

== Generate legacy-order-adaptor

=== Create JSON to POJO

. Start with some example `.json` generated from CDC event in xref:02-walkthrough.adoc#demonstratedbz[this section]
. Navigate to link:http://www.jsonschema2pojo.org/[this site] and paste in the json per screenshot
+
image::json2pojo.png[]
+
. Click the link to download the zip file
. expand into the `functions` directory
. Update tests to show JSON to POJO working


