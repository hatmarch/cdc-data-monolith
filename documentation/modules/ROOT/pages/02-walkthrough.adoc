= Demo Walkthrough
include::_attributes.adoc[]

[#legacy]
== Introduction to Legacy Application

. Show the coolstore without the inventory.  You can get the URL of the coolstore UI by running the following command
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
COOLSTORE_URL=http://$(oc get route coolstore-ui -o jsonpath='{.spec.host}' -n cdc-coolstore)
----
+
. Explain that the inventory comes from another system.  A legacy system.
. Navigate to the legacy system.  You can get the URL to the legacy application by running the following command in the demo shell:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
LEGACY_URL=http://$(oc get route www -o jsonpath='{.spec.host}' -n ${PROJECT_PREFIX}-dev)
----
+
. Show that the inventory database is currently empty by clicking on the "Inventory" navbar
+
image::legacy-inventory-navbar.png[]
+
. Show that the legacy system accepts an upload of inventory data by clicking on the `Home` navbar element
. Next click on `Choose file` button which should open up a window like so:
+
image::legacy-inventory-upload.png[]
+
. In the directory shown (`$DEMO_HOME/example`) choose the `coolstore-inventory.csv'
. Click `Load File`
. You should now see a screen of all the imported inventory.  
+
[IMPORTANT]
====
Keep this tab open as we will revisit this page later
====
+
image:loaded-inventory.png[]
+
. Reopen the `$COOLSTORE_URL` and notice that inventory has not been updated still

[#producerconsumer]
== Kafka: Producers, Consumers, and Groups

In this demo we want to show the notion of producers and consumers and ideally topics without any of the k8 overlays

See link:https://medium.com/@TimvanBaarsen/apache-kafka-cli-commands-cheat-sheet-a6f06eac01b#fe4f[here] for some commands that can be used.

For this we setup 4 terminals.  

. All four get setup in the same way.  Run the following command first in each:
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
# from the root of this repo
docker run -it -v ~/.kube:/home/jboss/.kube -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/cdc-data-monolith -w /workspaces/cdc-data-monolith quay.io/mhildenb/cdc-demo-shell /bin/zsh
----
+
. Once in the docker shell, ensure you are logged in (one you are logged into one you should be logged into all)
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc whoami
----
+
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
opentlc-mgr
----
+
. Next run the following command to setup your environment
+
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
. pass:[${DEMO_HOME}]/example/scripts/producer-consumer-setup.sh
----
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1 (Producer)::
+
--
Terminal one is for producing information on a topic.  Run the following command to get started

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic pass:[${TOPIC}]
----

NOTE: The terminal is ready to take input on the topic when you see the chevron (`>`)

--
Terminal 2::
+
--
Terminal 2 will be our first consumer in the group `consumer-group-1`

Run the following command to get it started:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic pass:[${TOPIC}] --group consumer-group-1
----
--
Terminal 3::
+
--
Terminal 3 will be our second consumer in the group `consumer-group-1`

Run the following command to get it started:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic pass:[${TOPIC}] --group consumer-group-1
----
--
Terminal 4::
+
--
Terminal 4 will be our first consumer in the group `consumer-group-solo`

Run the following command to get it started:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic pass:[${TOPIC}] --group consumer-group-solo
----
--
====
+
. Enter the following
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1 (Producer)::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
Test Event 1
----
--
====
+
. You should notice output in one of Terminal 2 or 3 and Terminal 4
+
image::four-terminal-producer-consumer.png[]
+
. Enter the following
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1 (Producer)::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
Test Event 2
----
--
====
+
. You should notice output in the same of Terminal 2 and 3 that received the event before and Terminal 4
. Next kill the consumer in either Terminal 2 or 3, whichever one got the previous two events, with kbd:[ctrl+c]
. Enter the following
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1 (Producer)::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
Test Event 3
----
--
====
+
. This time the other terminal of `consumer-group-1` should get the message as should Terminal 4
. Now kill the consumer in Terminal 4 with kbd:[ctrl+c]
. Enter the following:
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1 (Producer)::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
Test Event 4
----
--
====
+
. You should only see the event appear in the remaining connected terminal
. Finally, let's see how the consumer groups have gone.  Let's use the `kafka-consumer-groups` command to see:
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 4::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group consumer-group-solo
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group consumer-group-1
----
--
====
+
. Output should be similar to:
+
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
Consumer group 'consumer-group-solo' has no active members.

GROUP               TOPIC                    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
consumer-group-solo test-producer-consumer   0          16              17              1               -               -               -
----
+
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
GROUP            TOPIC                    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                                      HOST            CLIENT-ID
consumer-group-1 test-producer-consumer   0          17              17              0               consumer-consumer-group-1-1-c2d8588c-996e-47df-bb46-66d948d4a9ea /10.128.2.16    consumer-consumer-group-1-1
----
+
. Next, let's reset the offset of the only consumer group that is currently offline (has no active members)
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 4::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --reset-offsets --to-earliest --group consumer-group-solo --topic pass:[${TOPIC}] --execute
----

[.console-output]
[source,bash,subs="attributes+,+macros"]
----
GROUP                          TOPIC                          PARTITION  NEW-OFFSET     
consumer-group-solo            test-producer-consumer-1       0          0              
----
--
====
+
. Finally, let's restart a consumer of the `consumer-group-solo`
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 4::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it demo-kafka-0 -n pass:[${KAFKA_PROJECT}] -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic pass:[${TOPIC}] --group consumer-group-solo
----
--
====
+
. After a few seconds you should see the following output
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 4::
+
--
[.console-output]
[source,bash,subs="attributes+,+macros"]
----
Test Event 1
Test Event 2
Test Event 3
Test Event 4
Test Event 5
----
--
====
. Which shows that our offset has been reset to the start causing us to reread the events

[#strimzi]
== Strimzi and AMQStreams Operator

. Go to the {test-project} project, Developer Perspective
. Click on `+Add` select `Developer Catalog` and then start searching for `kafka` in the "Operator Backed" items
+
image::kafka-operator.png[]
+
. A page will show up, just click `Create`
. Show some of the options (perhaps even "YAML View") briefly, but create a kafka using the defaults, *setting name to `example`* then scrolling down and clicking `Create`
+
image::kafka-example-creation.png[]
+
. The Kafka cluster will start to appear

[#examplekafkaconnect]
== Kafka Connect Introduction

. Go to the `{test-project}` project, Developer Perspective
+
NOTE: The `example` kafka cluster should be fully running now
+
. Before doing anything, pick a shell and run the following `stern` command which we'll use to look at the plugins on the Connect instance
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
stern connect -n {test-project} -i "Added plugin"
----
--
====
+
. Click the book with the plus (`Quick Catalog Add`) and type `Connect` in the text box and then click `Create`
+
image:kafka-connect-catalog.png[]
+
. Then fill in the kafka connect form view per below **being extra careful to remove the tls certs** and **add the annotation* in YAML
+ 
image:kafka-connect-form.png[]
image:remove-certs.png[]
+
. Switch to YAML view and make sure to add the following annotation:
+
----
  annotations:
    strimzi.io/use-connector-resources: 'true'
----
+
image::kafka-connect-yaml.png[]
+
. Finally click `Create` to create the KakfaConnect
. After a few moments you should see the following in the `stern` shell (*Terminal 1*)
+
[.console-output]
----
+ example-connect-7856c88cf-mthmb â€º example-connect
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,423 INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,423 INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,424 INFO Added plugin 'org.apache.kafka.connect.tools.MockSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,424 INFO Added plugin 'org.apache.kafka.connect.tools.SchemaSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
#example-connect-7856c88cf-mthmb example-connect 2021-03-10 12:05:05,424 INFO Added plugin 'org.apache.kafka.connect.file.FileStreamSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]#
...
NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader) [main]
----
. Then, make the following two terminals visible and run the commands therein:
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal2::
+
--
Here we will remote to the connect instance

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc rsh deploy/example-connect
----

Once connected, run the following in the pod:

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
cd /tmp
echo "Line 1" >> test.txt
----
--
Terminal 3::
+
--
Here we will look to receive any updates on the topic that the file will be streaming to

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
oc exec -it example-kafka-0 -n {test-project} -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic {example-connector-topic} --from-beginning
----
--
====
. Next, use the `Quick Add Catalog` button to create a new `Kafka Connector`
+
image:new-kafka-connector.png[]
+
. Switch to `YAML View`
. Show that the name of the connector matches the Plugin log output from Terminal 1
** `org.apache.kafka.connect.file.FileStreamSourceConnector`
. Update the YAML as follows:
** Make sure the topic matches what is being consumed in *Terminal 3*
** Make sure the file matches what was just created in the `rsh` on *Terminal 2*
+
image::file-connector-yaml.png[]
+
. A few moments later, you should then see the following in *Terminal 3*
+
[.console-output]
----
{"schema":{"type":"string","optional":false},"payload":"Line 1"}
----
+
. Click `Search` and then start typing `kafkaconnnect` and select `KafkaConnector` from the drop-down
+
image:connector-search.png[]
+
. Click on the `file-stream-connector` and then the YAML view
+
image::connector-status.png[]
+
[NOTE]
====
Alternatively, you can following command in Terminal 2 (rsh)

[.console-input]
[source,bash,subs="attributes+,+macros"]
----
curl localhost:8083/connectors/file-stream-connector/status
----
====
. Next let's pump some text through the file
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 2 (rsh)::
+
--
[.console-input]
[source,bash,subs="attributes+,+macros"]
----
c=0; while (true); do ((c=c+1)); echo "Line ${c}" >> test.txt; sleep 2; done
----
--
====
+
. This should cause lines like the following to appear every 2 seconds in *Terminal 3*
+
[.console-output]
----
{"schema":{"type":"string","optional":false},"payload":"Line 2"}
{"schema":{"type":"string","optional":false},"payload":"Line 3"}
{"schema":{"type":"string","optional":false},"payload":"Line 4"}
{"schema":{"type":"string","optional":false},"payload":"Line 5"}
{"schema":{"type":"string","optional":false},"payload":"Line 6"}
{"schema":{"type":"string","optional":false},"payload":"Line 7"}
{"schema":{"type":"string","optional":false},"payload":"Line 8"}
{"schema":{"type":"string","optional":false},"payload":"Line 9"}
{"schema":{"type":"string","optional":false},"payload":"Line 10"}
----
+
. Delete the kafkaconnector and then notice that eventually (when the amq operator catches up) the topic goes silent
+
[tabs,subs="attributes+,+macros"]	
====	
Terminal 1::
+
--
----
oc delete -n {test-project} kafkaconnector/file-stream-connector
----
--
====


[#debeziumconnector]
== Enable Debezium Connector

=== Configure SQL Database

See last part of `ansible/demo/templates/configmap-data-sql.yaml.j2` and instructions link:https://debezium.io/documentation/reference/connectors/sqlserver.html#_enabling_cdc_on_the_sql_server_database[here]

[#demonstratedbz]
== Demonstrate Debezium

. Connect to database using Adminer per instructions xref:03-appendix.adoc#mssql[here]
. In another terminal watch where the order events will go
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc exec -it demo-kafka-0 -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic mssql-server-linux.dbo.Orders --from-beginning
----
+
. Go to adminer and create a new entry

== Generate legacy-order-adaptor

=== Create JSON to POJO

. Start with some example `.json` generated from CDC event in xref:02-walkthrough.adoc#demonstratedbz[this section]
. Navigate to link:http://www.jsonschema2pojo.org/[this site] and paste in the json per screenshot
+
image::json2pojo.png[]
+
. Click the link to download the zip file
. expand into the `functions` directory
. Update tests to show JSON to POJO working

[#deploylegacyconsumer]
=== Deploy Legacy Consumer

Run the following ansible command to create a deployment and configmap for the connector

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
ansible-playbook -i pass:[${DEMO_HOME}]/ansible/demo/inventory \
    pass:[${DEMO_HOME}]/ansible/demo/main.yaml \
    -e "ACTION=consumer_create" 
----
